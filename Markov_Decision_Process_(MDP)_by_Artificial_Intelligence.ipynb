{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Name: Markov Decision Process (MDP)  by Artificial Intelligence\n",
        "#Contributor: Rajeev singh sisodiya\n",
        "\n",
        "Project details:\n",
        "\n",
        "Markov Decision Process (MDP) is a mathematical framework used to model decision-making in situations where an agent interacts with an environment over a series of discrete time steps. MDPs are a fundamental concept in reinforcement learning and provide a formal structure for studying and solving problems related to decision making under uncertainty.\n",
        "\n",
        "In an MDP, the agent selects actions in various states, and the environment responds by transitioning the system to new states, providing rewards based on these actions. The key components of an MDP include states, actions, transition probabilities, rewards, and policies. MDPs are built on the Markov property, ensuring that the future state depends only on the current state and action, simplifying the modeling process.\n",
        "\n",
        "Value functions, such as the state-value and action-value functions, are fundamental to MDPs. These functions estimate the expected cumulative future rewards and guide the agent in making decisions. The Bellman equations capture the recursive relationships between values of states or state-action pairs, facilitating the computation of optimal policies.\n",
        "\n",
        "The pursuit of an optimal policy, one that maximizes cumulative rewards, is a central goal in MDPs. Algorithms like Q-learning and policy iteration are employed to find these optimal policies efficiently. MDPs have widespread applications in AI, ranging from robotics and autonomous systems to game playing and decision support systems. They provide a robust framework for modeling complex decision-making scenarios and have proven instrumental in advancing AI capabilities in dynamic and uncertain environments.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CiqSVsdmPJSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Typical Reinforcement Learning Cycle:\n",
        "\n",
        "Agent: The learner or decision-maker that interacts with the environment.\n",
        "Environment: The external system with which the agent interacts.\n",
        "\n",
        "State (S): A representation of the current situation of the environment. The agent perceives the current state of the environment (e.g., game board configuration, robot sensor readings).\n",
        "\n",
        "Action (A): The set of possible moves or decisions that the agent can make. Based on the state and its policy, the agent chooses an action to execute (e.g., move a piece, turn a knob).\n",
        "\n",
        "Reward (R): A numerical value indicating the immediate benefit or cost associated with an action in a given state.  The environment gives the agent a reward (positive or negative) based on the chosen action and resulting state change.\n",
        "\n",
        "Policy (π): A strategy or mapping from states to actions that the agent follows. The agent learns from the experience and updates its policy to improve future actions in similar situations."
      ],
      "metadata": {
        "id": "m6Z-pXDld9x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Environment class (replace with your specific environment implementation)\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        # Initialize environment state, actions, rewards, etc.\n",
        "        pass\n",
        "\n",
        "    def get_state(self):\n",
        "        # Return the current state of the environment\n",
        "        pass\n",
        "\n",
        "    def take_action(self, action):\n",
        "        # Perform the action and return the next state, reward, and done flag\n",
        "        pass\n",
        "\n",
        "# Agent class\n",
        "class Agent:\n",
        "    def __init__(self, environment):\n",
        "        self.environment = environment\n",
        "        # Initialize policy (e.g., a Q-table or function approximator)\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        # Choose an action based on the current policy and state\n",
        "        pass\n",
        "\n",
        "    def update_policy(self, state, action, reward, next_state, done):\n",
        "        # Update the policy based on the experience\n",
        "        pass\n",
        "\n",
        "# Reinforcement Learning loop\n",
        "def rl_loop(agent, environment, num_episodes):\n",
        "    for episode in range(num_episodes):\n",
        "        state = environment.reset()  # Reset environment to initial state\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state)  # Choose action\n",
        "            next_state, reward, done = environment.take_action(action)  # Take action\n",
        "            agent.update_policy(state, action, reward, next_state, done)  # Update policy\n",
        "            state = next_state  # Transition to the next state\n"
      ],
      "metadata": {
        "id": "pYobKbflhv1U"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The typical reinforcement learning cycle involves an agent interacting with an environment, making decisions, and learning from the feedback received in the form of rewards."
      ],
      "metadata": {
        "id": "ZSx77S4zjV-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # Simulate the environment dynamics\n",
        "        if action == \"left\":\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == \"right\":\n",
        "            self.state = min(4, self.state + 1)\n",
        "\n",
        "        # Provide a reward based on the new state\n",
        "        if self.state == 4:\n",
        "            reward = 1\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.q_values = {\"left\": 0, \"right\": 0}\n",
        "        self.epsilon = 0.1  # Exploration-exploitation trade-off\n",
        "\n",
        "    def select_action(self):\n",
        "        # Epsilon-greedy strategy for action selection\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice([\"left\", \"right\"])\n",
        "        else:\n",
        "            return max(self.q_values, key=self.q_values.get)\n",
        "\n",
        "    def update_q_values(self, action, reward):\n",
        "        # Simple Q-learning update rule\n",
        "        self.q_values[action] += 0.1 * (reward - self.q_values[action])\n",
        "\n",
        "# Main Reinforcement Learning Cycle\n",
        "def main():\n",
        "    env = Environment()\n",
        "    agent = Agent()\n",
        "\n",
        "    total_episodes = 10\n",
        "\n",
        "    for episode in range(total_episodes):\n",
        "        env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action()\n",
        "            state, reward, done = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            # Update Q-values based on the observed reward\n",
        "            agent.update_q_values(action, reward)\n",
        "\n",
        "            if done:\n",
        "                print(f\"Episode {episode + 1} - Total Reward: {total_reward}\")\n",
        "                break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsT3Z-4ah7b8",
        "outputId": "10bfc512-1943-4953-9f4c-737a5cd25f9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 - Total Reward: 1\n",
            "Episode 2 - Total Reward: 1\n",
            "Episode 3 - Total Reward: 1\n",
            "Episode 4 - Total Reward: 1\n",
            "Episode 5 - Total Reward: 1\n",
            "Episode 6 - Total Reward: 1\n",
            "Episode 7 - Total Reward: 1\n",
            "Episode 8 - Total Reward: 1\n",
            "Episode 9 - Total Reward: 1\n",
            "Episode 10 - Total Reward: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why do we need MDP? How does it differ from Bandit Problems?\n",
        "\n",
        "MDPs extend the concept of bandit problems to sequential decision-making scenarios. While bandit problems involve a single decision point, MDPs deal with situations where decisions have long-term consequences and are made sequentially over time.\n",
        "\n",
        "To illustrate why we need Markov Decision Processes (MDP) and how they differ from Bandit Problems, let's create a simple example in Python. We'll implement a Multi-Armed Bandit problem and then extend it to a Markov Decision Process scenario."
      ],
      "metadata": {
        "id": "giWPpo8beNpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Bandit:\n",
        "    def __init__(self, num_arms):\n",
        "        self.num_arms = num_arms\n",
        "        self.true_means = np.random.normal(0, 1, num_arms)\n",
        "        self.action_values = np.zeros(num_arms)\n",
        "        self.action_counts = np.zeros(num_arms)\n",
        "\n",
        "    def pull_arm(self, arm):\n",
        "        reward = np.random.normal(self.true_means[arm], 1)\n",
        "        self.action_counts[arm] += 1\n",
        "        self.action_values[arm] += (reward - self.action_values[arm]) / self.action_counts[arm]\n",
        "        return reward\n",
        "\n",
        "class MDPEnvironment:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.transition_probabilities = np.random.rand(num_states, num_actions, num_states)\n",
        "        self.transition_probabilities /= np.sum(self.transition_probabilities, axis=2, keepdims=True)\n",
        "        self.rewards = np.random.normal(0, 1, (num_states, num_actions))\n",
        "\n",
        "    def transition(self, state, action):\n",
        "        next_state = np.random.choice(self.num_states, p=self.transition_probabilities[state, action])\n",
        "        reward = self.rewards[state, action]\n",
        "        return next_state, reward\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, num_arms_or_actions):\n",
        "        self.num_arms_or_actions = num_arms_or_actions\n",
        "        self.action_values = np.zeros(num_arms_or_actions)\n",
        "        self.action_counts = np.zeros(num_arms_or_actions)\n",
        "\n",
        "    def select_action(self):\n",
        "        return np.argmax(self.action_values)\n",
        "\n",
        "    def update_action_value(self, action, reward):\n",
        "        self.action_counts[action] += 1\n",
        "        self.action_values[action] += (reward - self.action_values[action]) / self.action_counts[action]\n",
        "\n",
        "# Multi-Armed Bandit\n",
        "def bandit_example():\n",
        "    num_arms = 5\n",
        "    bandit = Bandit(num_arms)\n",
        "    agent = Agent(num_arms)\n",
        "    total_pulls = 1000\n",
        "\n",
        "    for _ in range(total_pulls):\n",
        "        action = agent.select_action()\n",
        "        reward = bandit.pull_arm(action)\n",
        "        agent.update_action_value(action, reward)\n",
        "\n",
        "    print(\"Multi-Armed Bandit Example:\")\n",
        "    print(f\"True Means: {bandit.true_means}\")\n",
        "    print(f\"Estimated Means: {agent.action_values}\\n\")\n",
        "\n",
        "# Markov Decision Process\n",
        "def mdp_example():\n",
        "    num_states = 5\n",
        "    num_actions = 2\n",
        "    environment = MDPEnvironment(num_states, num_actions)\n",
        "    agent = Agent(num_actions)\n",
        "    total_steps = 1000\n",
        "\n",
        "    state = np.random.randint(num_states)  # Initial state\n",
        "\n",
        "    for _ in range(total_steps):\n",
        "        action = agent.select_action()\n",
        "        next_state, reward = environment.transition(state, action)\n",
        "        agent.update_action_value(action, reward)\n",
        "        state = next_state\n",
        "\n",
        "    print(\"Markov Decision Process Example:\")\n",
        "    print(f\"Transition Probabilities:\\n{environment.transition_probabilities}\")\n",
        "    print(f\"Estimated Action Values: {agent.action_values}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    bandit_example()\n",
        "    mdp_example()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puuGycp8i9Nv",
        "outputId": "3184de41-423a-431e-dd86-666c19062d32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Armed Bandit Example:\n",
            "True Means: [ 1.1059011  -0.13737328  0.92130453  0.13559085 -0.38338092]\n",
            "Estimated Means: [1.08380994 0.         0.         0.         0.        ]\n",
            "\n",
            "Markov Decision Process Example:\n",
            "Transition Probabilities:\n",
            "[[[0.47880719 0.18778438 0.08312988 0.01692055 0.233358  ]\n",
            "  [0.29546345 0.22640316 0.10014194 0.23714636 0.1408451 ]]\n",
            "\n",
            " [[0.06522042 0.1487977  0.33506289 0.17528427 0.27563471]\n",
            "  [0.13487743 0.24304678 0.15147628 0.3111573  0.15944222]]\n",
            "\n",
            " [[0.13502925 0.29414492 0.23533696 0.11349399 0.22199488]\n",
            "  [0.0720785  0.22149613 0.04410567 0.28770461 0.37461509]]\n",
            "\n",
            " [[0.20867114 0.06058335 0.14213177 0.34124099 0.24737276]\n",
            "  [0.0617198  0.3369266  0.06315125 0.18024636 0.35795599]]\n",
            "\n",
            " [[0.1337484  0.42162935 0.06636137 0.2621761  0.11608477]\n",
            "  [0.30722961 0.30480013 0.18037476 0.17892341 0.02867209]]]\n",
            "Estimated Action Values: [-0.01017791  0.8029741 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Property:\n",
        "\n",
        "The Markov property states that the future state of the system is independent of the past given the present state. In the context of MDPs, it implies that the current state contains all the information needed to make decisions about the future.\n",
        "\n",
        "Below is a simple Python example to illustrate the Markov property in the context of a simple state transition process:"
      ],
      "metadata": {
        "id": "RIKBexGMeSP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "    def __init__(self):\n",
        "        # Initialize environment state, actions, rewards, etc.\n",
        "        pass\n",
        "\n",
        "    def get_state(self):\n",
        "        # Return the current state of the environment\n",
        "        pass\n",
        "\n",
        "    def take_action(self, action):\n",
        "        # Determine next state based only on current state and action\n",
        "        next_state = self._transition_function(self.current_state, action)\n",
        "\n",
        "        # Determine reward based only on current state, action, and next state\n",
        "        reward = self._reward_function(self.current_state, action, next_state)\n",
        "\n",
        "        # Update current state\n",
        "        self.current_state = next_state\n",
        "\n",
        "        return next_state, reward, self._is_terminal_state(next_state)  # Done flag based on terminal state\n",
        "\n",
        "    # ... (implementations for transition_function, reward_function, is_terminal_state)\n"
      ],
      "metadata": {
        "id": "0xUl7ernk2CU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MarkovChain:\n",
        "    def __init__(self, transition_matrix):\n",
        "        self.transition_matrix = transition_matrix\n",
        "        self.num_states = len(transition_matrix)\n",
        "\n",
        "    def transition(self, current_state):\n",
        "        # Simulate a state transition based on the Markov property\n",
        "        next_state = np.random.choice(self.num_states, p=self.transition_matrix[current_state])\n",
        "        return next_state\n",
        "\n",
        "def main():\n",
        "    # Define a transition matrix for a simple Markov Chain\n",
        "    # Each row represents the transition probabilities from the current state to all other states\n",
        "    transition_matrix = np.array([\n",
        "        [0.7, 0.3],\n",
        "        [0.2, 0.8]\n",
        "    ])\n",
        "\n",
        "    markov_chain = MarkovChain(transition_matrix)\n",
        "\n",
        "    # Simulate state transitions while maintaining the Markov property\n",
        "    current_state = np.random.choice(markov_chain.num_states)  # Initial state\n",
        "\n",
        "    num_steps = 10\n",
        "    print(\"State transitions:\")\n",
        "    for _ in range(num_steps):\n",
        "        print(f\"Step: {_}, Current State: {current_state}\")\n",
        "        current_state = markov_chain.transition(current_state)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhKBDLqoksXY",
        "outputId": "546699fc-b0b3-4e3b-d46c-7119ec98de01"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State transitions:\n",
            "Step: 0, Current State: 0\n",
            "Step: 1, Current State: 0\n",
            "Step: 2, Current State: 0\n",
            "Step: 3, Current State: 0\n",
            "Step: 4, Current State: 0\n",
            "Step: 5, Current State: 0\n",
            "Step: 6, Current State: 0\n",
            "Step: 7, Current State: 0\n",
            "Step: 8, Current State: 1\n",
            "Step: 9, Current State: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Episodic and Continuous Tasks:\n",
        "\n",
        "Episodic tasks: Have a well-defined starting and ending point (e.g., games with rounds).\n",
        "Continuous tasks: Have no natural termination point (e.g., a robot navigating in an environment).\n",
        "\n",
        "To illustrate the concepts of episodic and continuous tasks in the context of reinforcement learning, let's create a simple Python example. We'll define two environments, one representing an episodic task and the other a continuous task."
      ],
      "metadata": {
        "id": "pY93hxVKeWuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Episodic Tasks:"
      ],
      "metadata": {
        "id": "SZG568RinqY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinforcement Learning loop\n",
        "def rl_loop(agent, environment, num_episodes):\n",
        "    for episode in range(num_episodes):\n",
        "        state = environment.reset()  # Reset environment to initial state\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state)  # Choose action\n",
        "            next_state, reward, done = environment.take_action(action)  # Take action\n",
        "            agent.update_policy(state, action, reward, next_state, done)  # Update policy\n",
        "            state = next_state  # Transition to the next state\n",
        "\n",
        "def rl_loop_episodic(agent, environment, num_episodes):\n",
        "    for episode in range(num_episodes):\n",
        "        state = environment.reset()  # Reset to initial state at the start of each episode\n",
        "        done = False\n",
        "        episode_reward = 0  # Track total reward per episode\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done = environment.take_action(action)\n",
        "            agent.update_policy(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "        # Print or log episode reward statistics\n",
        "        print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n"
      ],
      "metadata": {
        "id": "0aNU7eXSnf12"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous Tasks:"
      ],
      "metadata": {
        "id": "8mhgcrrxniOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinforcement Learning loop\n",
        "def rl_loop(agent, environment, num_episodes):\n",
        "    for episode in range(num_episodes):\n",
        "        state = environment.reset()  # Reset environment to initial state\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state)  # Choose action\n",
        "            next_state, reward, done = environment.take_action(action)  # Take action\n",
        "            agent.update_policy(state, action, reward, next_state, done)  # Update policy\n",
        "            state = next_state  # Transition to the next state\n",
        "\n",
        "\n",
        "def rl_loop_continuous(agent, environment, num_steps):\n",
        "    state = environment.reset()  # Initial state\n",
        "    done = False\n",
        "    total_reward = 0  # Track total reward across steps\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done = environment.take_action(action)\n",
        "        agent.update_policy(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:  # Restart if the task ends prematurely\n",
        "            state = environment.reset()\n",
        "            done = False\n",
        "\n",
        "    # Print or log total reward statistics\n",
        "    print(f\"Total Reward over {num_steps} steps: {total_reward}\")\n"
      ],
      "metadata": {
        "id": "QsYuW-2Ymo4h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this below machine learning code example , EpisodicTask represents an episodic task with a fixed number of steps per episode, and ContinuousTask represents a continuous task where the episode doesn't have a fixed length. The tasks have a state space of 10 states, and the agent takes random actions (0 or 1) at each step.\n",
        "\n",
        "The run_task function is used to simulate the agent interacting with the task for a specified number of steps, and it returns the total reward obtained during the interaction.\n",
        "\n",
        "The main function demonstrates running both episodic and continuous tasks and printing the total rewards obtained."
      ],
      "metadata": {
        "id": "j9V6e4V8oaB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class EpisodicTask:\n",
        "    def __init__(self, num_states):\n",
        "        self.num_states = num_states\n",
        "        self.current_state = 0\n",
        "        self.episode_length = 5\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = 0\n",
        "        self.current_step = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # In an episodic task, the episode terminates after a fixed number of steps\n",
        "        self.current_step += 1\n",
        "        if self.current_step >= self.episode_length:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        # Reward is 1 if the action leads to the last state, 0 otherwise\n",
        "        reward = 1 if self.current_state == self.num_states - 1 else 0\n",
        "\n",
        "        # Transition to the next state\n",
        "        if not done:\n",
        "            self.current_state = min(self.num_states - 1, self.current_state + action)\n",
        "\n",
        "        return self.current_state, reward, done\n",
        "\n",
        "class ContinuousTask:\n",
        "    def __init__(self, num_states):\n",
        "        self.num_states = num_states\n",
        "        self.current_state = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # In a continuous task, the episode does not have a fixed length\n",
        "        self.current_state = min(self.num_states - 1, max(0, self.current_state + action))\n",
        "\n",
        "        # Reward is 1 if the action leads to the last state, 0 otherwise\n",
        "        reward = 1 if self.current_state == self.num_states - 1 else 0\n",
        "\n",
        "        # For continuous tasks, we don't have a concept of \"done\"\n",
        "        done = False\n",
        "\n",
        "        return self.current_state, reward, done\n",
        "\n",
        "def run_task(task, total_steps):\n",
        "    total_reward = 0\n",
        "    task.reset()\n",
        "\n",
        "    for _ in range(total_steps):\n",
        "        action = np.random.randint(2)  # Random action (0 or 1)\n",
        "        state, reward, done = task.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "def main():\n",
        "    num_states = 10\n",
        "    total_steps = 100\n",
        "\n",
        "    # Run an episodic task\n",
        "    episodic_task = EpisodicTask(num_states)\n",
        "    episodic_reward = run_task(episodic_task, total_steps)\n",
        "    print(f\"Episodic Task Reward: {episodic_reward}\")\n",
        "\n",
        "    # Run a continuous task\n",
        "    continuous_task = ContinuousTask(num_states)\n",
        "    continuous_reward = run_task(continuous_task, total_steps)\n",
        "    print(f\"Continuous Task Reward: {continuous_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTm1ECuEmdeo",
        "outputId": "9018719f-1d70-4a19-8175-bf30e2c57e3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episodic Task Reward: 0\n",
            "Continuous Task Reward: 85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Returns using Discount Factor in Continuous Task:\n",
        "\n",
        "The return is the cumulative sum of discounted rewards over time. The discount factor (γ) represents the importance of future rewards, and it typically ranges between 0 and 1. Returns with a discount factor in a continuous task can be calculated as the sum of discounted rewards over time. Here's a simple Python example to illustrate this concept:"
      ],
      "metadata": {
        "id": "skaYlsTVea3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ContinuousTask:\n",
        "    def __init__(self, num_states, discount_factor=0.9):\n",
        "        self.num_states = num_states\n",
        "        self.current_state = 0\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # In a continuous task, the episode does not have a fixed length\n",
        "        self.current_state = min(self.num_states - 1, max(0, self.current_state + action))\n",
        "\n",
        "        # Reward is 1 if the action leads to the last state, 0 otherwise\n",
        "        reward = 1 if self.current_state == self.num_states - 1 else 0\n",
        "\n",
        "        return self.current_state, reward\n",
        "\n",
        "def calculate_discounted_return(rewards, discount_factor):\n",
        "    discounted_return = 0\n",
        "    discount = 1.0\n",
        "\n",
        "    for reward in rewards:\n",
        "        discounted_return += discount * reward\n",
        "        discount *= discount_factor\n",
        "\n",
        "    return discounted_return\n",
        "\n",
        "def run_continuous_task(task, total_steps):\n",
        "    rewards = []\n",
        "    task.reset()\n",
        "\n",
        "    for _ in range(total_steps):\n",
        "        action = np.random.randint(2)  # Random action (0 or 1)\n",
        "        state, reward = task.step(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def main():\n",
        "    num_states = 10\n",
        "    total_steps = 5\n",
        "\n",
        "    discount_factor = 0.9\n",
        "    continuous_task = ContinuousTask(num_states, discount_factor)\n",
        "\n",
        "    rewards = run_continuous_task(continuous_task, total_steps)\n",
        "\n",
        "    print(\"Rewards:\", rewards)\n",
        "    discounted_return = calculate_discounted_return(rewards, discount_factor)\n",
        "    print(f\"Discounted Return: {discounted_return:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccZY2oy4veOv",
        "outputId": "bd5e7b1c-7de8-4685-eec4-b57f844a00ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rewards: [0, 0, 0, 0, 0]\n",
            "Discounted Return: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policies:\n",
        "\n",
        "A policy (π) is a mapping from states to actions, indicating what action to take in each state. Policies can be deterministic or stochastic.\n",
        "\n",
        "Policy Flexibility: RL algorithms can learn both deterministic and stochastic policies, depending on the problem and the desired exploration-exploitation balance.\n",
        "\n",
        "Policy Updates: Reinforcement Learning algorithms update policies based on experience to improve the agent's decision-making over time.\n",
        "\n",
        "Action Probability Distribution: Stochastic policies offer the potential to explore different actions and potentially discover better strategies, even if they initially appear less promising."
      ],
      "metadata": {
        "id": "z19_bcUmebx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deterministic Policies:\n",
        "\n",
        "Policy Representation: A dictionary maps states to specific actions, indicating the action to take deterministically in each state.\n",
        "\n",
        "Action Selection: The act_deterministic function simply looks up the state in the policy dictionary and returns the corresponding action."
      ],
      "metadata": {
        "id": "XEi1VUgXwPyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deterministic policy example using a dictionary\n",
        "deterministic_policy = {\n",
        "    \"state1\": \"action1\",\n",
        "    \"state2\": \"action2\",\n",
        "    # ... more state-action mappings\n",
        "}\n",
        "\n",
        "def act_deterministic(state, policy):\n",
        "    return policy[state]  # Directly retrieve the action for a given state\n"
      ],
      "metadata": {
        "id": "D1vM4XMCwSlr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Policies:\n",
        "\n",
        "Policy Representation: A dictionary maps states to dictionaries of action probabilities, representing the probability of taking each action in a given state.\n",
        "Action Selection: The act_stochastic function uses random.choices to randomly select an action based on the probabilities defined in the policy for the current state."
      ],
      "metadata": {
        "id": "vAewMssSwnY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stochastic policy example using a dictionary of action probabilities\n",
        "stochastic_policy = {\n",
        "    \"state1\": {\"action1\": 0.8, \"action2\": 0.2},\n",
        "    \"state2\": {\"action1\": 0.3, \"action2\": 0.7},\n",
        "    # ... more state-action probability mappings\n",
        "}\n",
        "\n",
        "import random\n",
        "\n",
        "def act_stochastic(state, policy):\n",
        "    action_probabilities = policy[state]\n",
        "    action = random.choices(list(action_probabilities.keys()), weights=list(action_probabilities.values()))[0]\n",
        "    return action  # Select action probabilistically\n"
      ],
      "metadata": {
        "id": "vcL-qWW9xCLg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Below is an example in Python illustrating deterministic and stochastic policies in the context of a simple environment with discrete states and actions:"
      ],
      "metadata": {
        "id": "w7C0QFSYxii3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleEnvironment:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "    def step(self, state, action):\n",
        "        # Simulate a simple environment with a fixed reward for each action\n",
        "        reward = np.random.normal(0, 1)\n",
        "        next_state = (state + 1) % self.num_states\n",
        "        return next_state, reward\n",
        "\n",
        "class DeterministicPolicy:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.policy = np.zeros(num_states, dtype=int)\n",
        "\n",
        "    def set_policy(self, state, action):\n",
        "        self.policy[state] = action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        return self.policy[state]\n",
        "\n",
        "class StochasticPolicy:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.policy_matrix = np.random.rand(num_states, num_actions)\n",
        "        self.policy_matrix /= np.sum(self.policy_matrix, axis=1, keepdims=True)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        return np.random.choice(self.num_actions, p=self.policy_matrix[state])\n",
        "\n",
        "def run_episode(environment, policy, num_steps):\n",
        "    total_reward = 0\n",
        "    state = 0\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        action = policy.get_action(state)\n",
        "        next_state, reward = environment.step(state, action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "def main():\n",
        "    num_states = 5\n",
        "    num_actions = 3\n",
        "    num_steps = 10\n",
        "\n",
        "    environment = SimpleEnvironment(num_states, num_actions)\n",
        "\n",
        "    # Deterministic Policy\n",
        "    deterministic_policy = DeterministicPolicy(num_states, num_actions)\n",
        "    for state in range(num_states):\n",
        "        action = np.random.randint(num_actions)\n",
        "        deterministic_policy.set_policy(state, action)\n",
        "\n",
        "    total_reward_det = run_episode(environment, deterministic_policy, num_steps)\n",
        "    print(f\"Deterministic Policy - Total Reward: {total_reward_det:.2f}\")\n",
        "\n",
        "    # Stochastic Policy\n",
        "    stochastic_policy = StochasticPolicy(num_states, num_actions)\n",
        "\n",
        "    total_reward_stoch = run_episode(environment, stochastic_policy, num_steps)\n",
        "    print(f\"Stochastic Policy - Total Reward: {total_reward_stoch:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWrD7yBqxjzA",
        "outputId": "42e0dafa-7492-4f6f-d41e-888b791b07a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deterministic Policy - Total Reward: -0.18\n",
            "Stochastic Policy - Total Reward: 0.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value Functions:\n",
        "\n",
        "Value functions estimate the expected cumulative future rewards of being in a certain state or taking a certain action.\n",
        "\n",
        "Value Function Types:\n",
        "\n",
        "State-value function (V(s)): Estimates the value of being in a state.\n",
        "\n",
        "Action-value function (Q(s, a)): Estimates the value of taking a specific action in a state.\n",
        "\n",
        "Iterative Updates: RL algorithms iteratively update these value functions based on experience, gradually improving their accuracy.\n",
        "\n",
        "Temporal Difference Learning: Many algorithms use techniques like TD(0), SARSA, or Q-learning to update value functions based on the difference between estimated and actual rewards.\n",
        "\n",
        "Function Approximation: For large or continuous state/action spaces, value functions can be approximated using function approximators like neural networks or linear regression."
      ],
      "metadata": {
        "id": "QDz4z17Dept9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a simple example in Python to illustrate value functions in the context of a Markov Decision Process (MDP):"
      ],
      "metadata": {
        "id": "EepoXRmi1Lc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleMDP:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.transition_matrix = np.random.rand(num_states, num_actions, num_states)\n",
        "        self.transition_matrix /= np.sum(self.transition_matrix, axis=2, keepdims=True)\n",
        "        self.rewards = np.random.normal(0, 1, (num_states, num_actions))\n",
        "\n",
        "class ValueFunctions:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.state_values = np.zeros(num_states)\n",
        "        self.action_values = np.zeros((num_states, num_actions))\n",
        "\n",
        "    def update_state_value(self, state, value):\n",
        "        self.state_values[state] = value\n",
        "\n",
        "    def update_action_value(self, state, action, value):\n",
        "        self.action_values[state, action] = value\n",
        "\n",
        "def value_iteration(mdp, value_functions, discount_factor, num_iterations=100):\n",
        "    for _ in range(num_iterations):\n",
        "        # Update state values\n",
        "        for state in range(mdp.num_states):\n",
        "            max_action_value = max(value_functions.action_values[state, :])\n",
        "            value_functions.update_state_value(state, max_action_value)\n",
        "\n",
        "        # Update action values\n",
        "        for state in range(mdp.num_states):\n",
        "            for action in range(mdp.num_actions):\n",
        "                next_state_values = np.sum(\n",
        "                    mdp.transition_matrix[state, action, :] * value_functions.state_values * discount_factor\n",
        "                )\n",
        "                value_functions.update_action_value(state, action, mdp.rewards[state, action] + next_state_values)\n",
        "\n",
        "def main():\n",
        "    num_states = 4\n",
        "    num_actions = 2\n",
        "    discount_factor = 0.9\n",
        "\n",
        "    mdp = SimpleMDP(num_states, num_actions)\n",
        "    value_functions = ValueFunctions(num_states, num_actions)\n",
        "\n",
        "    value_iteration(mdp, value_functions, discount_factor)\n",
        "\n",
        "    print(\"State Values:\")\n",
        "    print(value_functions.state_values)\n",
        "\n",
        "    print(\"\\nAction Values:\")\n",
        "    print(value_functions.action_values)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_F8fqQT1AUa",
        "outputId": "031f5070-a53f-4407-decd-6ab0bea1708f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Values:\n",
            "[0.9548376  0.07993539 2.08145235 1.61207434]\n",
            "\n",
            "Action Values:\n",
            "[[ 0.95484135  0.03261043]\n",
            " [ 0.07993914 -0.18624919]\n",
            " [-1.29354857  2.0814561 ]\n",
            " [ 1.61207808  0.45627135]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# State-value Function (V(s)):\n",
        "\n",
        "The expected return starting from a given state under a specific policy."
      ],
      "metadata": {
        "id": "lRd_Cwn4erjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# State-value function example using a dictionary\n",
        "state_values = {\n",
        "    \"state1\": 0.5,\n",
        "    \"state2\": 1.2,\n",
        "    # ... more state-value pairs\n",
        "}\n",
        "\n",
        "def estimate_state_value(state, state_values):\n",
        "    return state_values[state]  # Retrieve the estimated value for a given state\n"
      ],
      "metadata": {
        "id": "xiCC2S1d0LAi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Action-value Function (Q(s, a)):\n",
        "\n",
        "The expected return starting from a given state, taking a specific action, and following a specific policy thereafter."
      ],
      "metadata": {
        "id": "mgfPZZexey8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Action-value function example using a nested dictionary\n",
        "action_values = {\n",
        "    \"state1\": {\"action1\": 0.3, \"action2\": 1.0},\n",
        "    \"state2\": {\"action1\": 1.5, \"action2\": 0.8},\n",
        "    # ... more state-action value pairs\n",
        "}\n",
        "\n",
        "def estimate_action_value(state, action, action_values):\n",
        "    return action_values[state][action]  # Retrieve the estimated value for a given state-action pair\n"
      ],
      "metadata": {
        "id": "aqM_y3pY0Q81"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bellman Equation:\n",
        "\n",
        "Describes the relationship between the value of a state (or state-action pair) and the values of its successor states (or state-action pairs).\n",
        "\n",
        "Recursive Relationship: The Bellman Equation establishes a recursive relationship between the value of a state or state-action pair and the values of its possible successor states or state-action pairs.\n",
        "\n",
        "Iterative Updates: RL algorithms use this relationship to iteratively update value functions, gradually converging towards optimal values.\n",
        "\n",
        "Dynamic Programming and Temporal Difference Learning: The Bellman Equation forms the basis for both dynamic programming methods (e.g., Value Iteration, Policy Iteration) and temporal difference learning algorithms (e.g., Q-learning, SARSA)."
      ],
      "metadata": {
        "id": "8HtDoFYue0tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# State-Value Bellman Equation (V(s))"
      ],
      "metadata": {
        "id": "X5DBKPko4eqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bellman_equation_v(state, state_values, environment, gamma):\n",
        "    # Calculate expected value of successor states\n",
        "    expected_value = 0\n",
        "    for next_state, prob in environment.get_possible_next_states(state).items():\n",
        "        reward = environment.get_reward(state, next_state)\n",
        "        expected_value += prob * (reward + gamma * state_values[next_state])\n",
        "\n",
        "    return expected_value  # This represents V(s) according to the Bellman Equation\n"
      ],
      "metadata": {
        "id": "BU2xkQ8o4kL7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Action-Value Bellman Equation (Q(s, a))\n",
        "\n"
      ],
      "metadata": {
        "id": "eXBy1bui4nRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bellman_equation_q(state, action, action_values, environment, gamma):\n",
        "    # Calculate expected value of taking action in state\n",
        "    expected_value = 0\n",
        "    for next_state, prob in environment.get_possible_next_states(state, action).items():\n",
        "        reward = environment.get_reward(state, action, next_state)\n",
        "        max_next_q = max(action_values[next_state].values())  # Find max Q-value over possible next actions\n",
        "        expected_value += prob * (reward + gamma * max_next_q)\n",
        "\n",
        "    return expected_value  # This represents Q(s, a) according to the Bellman Equation\n"
      ],
      "metadata": {
        "id": "JpLJ3GsX4rs5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a Python example to illustrate the Bellman equation in the context of a Markov Decision Process (MDP):"
      ],
      "metadata": {
        "id": "xMJAIhgR4x7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleMDP:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.transition_matrix = np.random.rand(num_states, num_actions, num_states)\n",
        "        self.transition_matrix /= np.sum(self.transition_matrix, axis=2, keepdims=True)\n",
        "        self.rewards = np.random.normal(0, 1, (num_states, num_actions))\n",
        "\n",
        "class BellmanEquationSolver:\n",
        "    def __init__(self, mdp, discount_factor):\n",
        "        self.mdp = mdp\n",
        "        self.discount_factor = discount_factor\n",
        "        self.state_values = np.zeros(mdp.num_states)\n",
        "        self.action_values = np.zeros((mdp.num_states, mdp.num_actions))\n",
        "\n",
        "    def solve_bellman_equation(self, num_iterations=100):\n",
        "        for _ in range(num_iterations):\n",
        "            # Update state values using the Bellman equation\n",
        "            for state in range(self.mdp.num_states):\n",
        "                max_action_value = max(self.action_values[state, :])\n",
        "                self.state_values[state] = max_action_value\n",
        "\n",
        "            # Update action values using the Bellman equation\n",
        "            for state in range(self.mdp.num_states):\n",
        "                for action in range(self.mdp.num_actions):\n",
        "                    next_state_values = np.sum(\n",
        "                        self.mdp.transition_matrix[state, action, :] * self.state_values * self.discount_factor\n",
        "                    )\n",
        "                    self.action_values[state, action] = self.mdp.rewards[state, action] + next_state_values\n",
        "\n",
        "def main():\n",
        "    num_states = 4\n",
        "    num_actions = 2\n",
        "    discount_factor = 0.9\n",
        "\n",
        "    mdp = SimpleMDP(num_states, num_actions)\n",
        "    bellman_solver = BellmanEquationSolver(mdp, discount_factor)\n",
        "\n",
        "    bellman_solver.solve_bellman_equation()\n",
        "\n",
        "    print(\"State Values:\")\n",
        "    print(bellman_solver.state_values)\n",
        "\n",
        "    print(\"\\nAction Values:\")\n",
        "    print(bellman_solver.action_values)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj6tES0P46EX",
        "outputId": "a7fa4a57-7d05-4526-bb1f-2e510a17372c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Values:\n",
            "[13.6435754  13.61821393 11.64222591 14.75321899]\n",
            "\n",
            "Action Values:\n",
            "[[11.50522915 13.64361567]\n",
            " [13.61825421 13.18706319]\n",
            " [11.64226619 10.96563128]\n",
            " [12.2103402  14.75325926]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimal Policy:\n",
        "\n",
        "The policy that maximizes the expected return in every state.\n",
        "\n",
        "Value Function-Based Policy: Optimal policies are often derived from learned value functions, as the optimal action in a state typically corresponds to the action with the highest value.\n",
        "\n",
        "Dynamic Programming or Temporal Difference Learning: Algorithms like Value Iteration, Policy Iteration, Q-learning, and SARSA can be used to learn optimal value functions and policies iteratively.\n",
        "\n",
        "Exploration-Exploitation Trade-off: Stochastic policies balance exploration (trying new actions) with exploitation (choosing actions known to be good), which is crucial for effective learning in uncertain environments."
      ],
      "metadata": {
        "id": "8NuCyhSDe6bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deterministic Optimal Policy:\n",
        "\n"
      ],
      "metadata": {
        "id": "9h256aAYBgD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal policy represented as a dictionary mapping states to actions\n",
        "optimal_policy = {}\n",
        "\n",
        "# Function to find the action that maximizes the expected value in a state\n",
        "def find_optimal_action(state, action_values):\n",
        "    return max(action_values[state], key=action_values[state].get)  # Find the action with the highest Q-value\n",
        "\n",
        "# Derive the optimal policy based on the learned action-value function\n",
        "for state in action_values:\n",
        "    optimal_policy[state] = find_optimal_action(state, action_values)\n"
      ],
      "metadata": {
        "id": "DZ9ZY97GBlvI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Optimal Policy:"
      ],
      "metadata": {
        "id": "wsYmO2o8BrwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Optimal policy represented as a dictionary mapping states to action probabilities\n",
        "optimal_policy = {}\n",
        "\n",
        "# Function to assign probabilities to actions based on their Q-values\n",
        "def assign_action_probabilities(state, action_values):\n",
        "    action_probs = {}\n",
        "    total_q = sum(action_values[state].values())  # Normalize probabilities\n",
        "    for action, q_value in action_values[state].items():\n",
        "        action_probs[action] = q_value / total_q\n",
        "    return action_probs\n",
        "\n",
        "# Derive the optimal policy with probabilities\n",
        "for state in action_values:\n",
        "    optimal_policy[state] = assign_action_probabilities(state, action_values)\n"
      ],
      "metadata": {
        "id": "1IGQsiBIBwDV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a Python example to find the optimal policy in the context of a Markov Decision Process (MDP). We'll use the policy iteration algorithm to iteratively improve the policy until it converges to the optimal policy:"
      ],
      "metadata": {
        "id": "pNFXunVqCRJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleMDP:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.transition_matrix = np.random.rand(num_states, num_actions, num_states)\n",
        "        self.transition_matrix /= np.sum(self.transition_matrix, axis=2, keepdims=True)\n",
        "        self.rewards = np.random.normal(0, 1, (num_states, num_actions))\n",
        "\n",
        "class OptimalPolicySolver:\n",
        "    def __init__(self, mdp, discount_factor):\n",
        "        self.mdp = mdp\n",
        "        self.discount_factor = discount_factor\n",
        "        self.policy = np.random.randint(0, mdp.num_actions, mdp.num_states)\n",
        "\n",
        "    def evaluate_policy(self):\n",
        "        state_values = np.zeros(self.mdp.num_states)\n",
        "        tol = 1e-6  # Convergence tolerance\n",
        "\n",
        "        while True:\n",
        "            prev_state_values = state_values.copy()\n",
        "\n",
        "            # Evaluate state values using the current policy\n",
        "            for state in range(self.mdp.num_states):\n",
        "                action = self.policy[state]\n",
        "                next_state_values = np.sum(\n",
        "                    self.mdp.transition_matrix[state, action, :] * state_values * self.discount_factor\n",
        "                )\n",
        "                state_values[state] = self.mdp.rewards[state, action] + next_state_values\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.max(np.abs(state_values - prev_state_values)) < tol:\n",
        "                break\n",
        "\n",
        "        return state_values\n",
        "\n",
        "    def improve_policy(self, state_values):\n",
        "        # Improve policy using greedy strategy\n",
        "        for state in range(self.mdp.num_states):\n",
        "            action_values = np.zeros(self.mdp.num_actions)\n",
        "            for action in range(self.mdp.num_actions):\n",
        "                next_state_values = np.sum(\n",
        "                    self.mdp.transition_matrix[state, action, :] * state_values * self.discount_factor\n",
        "                )\n",
        "                action_values[action] = self.mdp.rewards[state, action] + next_state_values\n",
        "\n",
        "            # Update policy to the action that maximizes expected return\n",
        "            self.policy[state] = np.argmax(action_values)\n",
        "\n",
        "    def solve_optimal_policy(self, max_iterations=100):\n",
        "        for _ in range(max_iterations):\n",
        "            state_values = self.evaluate_policy()\n",
        "            self.improve_policy(state_values)\n",
        "\n",
        "def main():\n",
        "    num_states = 4\n",
        "    num_actions = 2\n",
        "    discount_factor = 0.9\n",
        "\n",
        "    mdp = SimpleMDP(num_states, num_actions)\n",
        "    optimal_policy_solver = OptimalPolicySolver(mdp, discount_factor)\n",
        "\n",
        "    optimal_policy_solver.solve_optimal_policy()\n",
        "\n",
        "    print(\"Optimal Policy:\")\n",
        "    print(optimal_policy_solver.policy)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUBCf730CSXL",
        "outputId": "82294297-ed57-4846-c279-ce4d4462d0f7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bellman Optimality Equation:\n",
        "\n",
        "Describes the relationship between the optimal value of a state (or state-action pair) and the values of its successor states (or state-action pairs) under the optimal policy.\n",
        "\n",
        " The Bellman Optimality Equation expresses the relationship between the optimal value of a state (or state-action pair) and the values of its successor states (or state-action pairs) under the optimal policy. The following Python example illustrates how to solve for the optimal values using the Bellman Optimality Equation in the context of a Markov Decision Process (MDP):"
      ],
      "metadata": {
        "id": "uBqarjhLe-h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleMDP:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.transition_matrix = np.random.rand(num_states, num_actions, num_states)\n",
        "        self.transition_matrix /= np.sum(self.transition_matrix, axis=2, keepdims=True)\n",
        "        self.rewards = np.random.normal(0, 1, (num_states, num_actions))\n",
        "\n",
        "class BellmanOptimalitySolver:\n",
        "    def __init__(self, mdp, discount_factor):\n",
        "        self.mdp = mdp\n",
        "        self.discount_factor = discount_factor\n",
        "        self.optimal_state_values = np.zeros(mdp.num_states)\n",
        "        self.optimal_action_values = np.zeros((mdp.num_states, mdp.num_actions))\n",
        "\n",
        "    def solve_bellman_optimality_equation(self, num_iterations=100):\n",
        "        for _ in range(num_iterations):\n",
        "            # Update state values using the Bellman Optimality Equation\n",
        "            for state in range(self.mdp.num_states):\n",
        "                max_action_value = max(self.optimal_action_values[state, :])\n",
        "                self.optimal_state_values[state] = max_action_value\n",
        "\n",
        "            # Update action values using the Bellman Optimality Equation\n",
        "            for state in range(self.mdp.num_states):\n",
        "                for action in range(self.mdp.num_actions):\n",
        "                    next_state_values = np.sum(\n",
        "                        self.mdp.transition_matrix[state, action, :] * self.optimal_state_values * self.discount_factor\n",
        "                    )\n",
        "                    self.optimal_action_values[state, action] = self.mdp.rewards[state, action] + next_state_values\n",
        "\n",
        "    def get_optimal_policy(self):\n",
        "        # Derive the optimal policy from the computed optimal action values\n",
        "        optimal_policy = np.argmax(self.optimal_action_values, axis=1)\n",
        "        return optimal_policy\n",
        "\n",
        "def main():\n",
        "    num_states = 4\n",
        "    num_actions = 2\n",
        "    discount_factor = 0.9\n",
        "\n",
        "    mdp = SimpleMDP(num_states, num_actions)\n",
        "    bellman_optimality_solver = BellmanOptimalitySolver(mdp, discount_factor)\n",
        "\n",
        "    bellman_optimality_solver.solve_bellman_optimality_equation()\n",
        "\n",
        "    print(\"Optimal State Values:\")\n",
        "    print(bellman_optimality_solver.optimal_state_values)\n",
        "\n",
        "    print(\"\\nOptimal Action Values:\")\n",
        "    print(bellman_optimality_solver.optimal_action_values)\n",
        "\n",
        "    optimal_policy = bellman_optimality_solver.get_optimal_policy()\n",
        "    print(\"\\nOptimal Policy:\")\n",
        "    print(optimal_policy)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65m1OErbC2mV",
        "outputId": "78191f51-953c-4175-c413-292ebf9674d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal State Values:\n",
            "[4.96736232 5.55588727 6.36279299 4.36417633]\n",
            "\n",
            "Optimal Action Values:\n",
            "[[4.96737815 3.88710943]\n",
            " [4.85222433 5.5559031 ]\n",
            " [6.36280881 4.9245516 ]\n",
            " [4.36419215 4.07047531]]\n",
            "\n",
            "Optimal Policy:\n",
            "[0 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "State-Value Bellman Optimality Equation (V(s))*\n",
        "\n"
      ],
      "metadata": {
        "id": "cIlcWD5SDISk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bellman_optimality_equation_v(state, environment, gamma):\n",
        "    # Find the optimal action in the state under the optimal policy\n",
        "    optimal_action = find_optimal_action(state, action_values)  # Assuming action_values represent the optimal Q-function\n",
        "\n",
        "    # Calculate the expected value of the optimal action\n",
        "    expected_value = 0\n",
        "    for next_state, prob in environment.get_possible_next_states(state, optimal_action).items():\n",
        "        reward = environment.get_reward(state, optimal_action, next_state)\n",
        "        expected_value += prob * (reward + gamma * state_values[next_state])  # Use state_values to approximate V*(s)\n",
        "\n",
        "    return expected_value  # This represents V*(s) according to the Bellman Optimality Equation\n"
      ],
      "metadata": {
        "id": "OdXhWWF6DKtT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Action-Value Bellman Optimality Equation (Q(s, a))*"
      ],
      "metadata": {
        "id": "HsR_RnDtDQYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bellman_optimality_equation_q(state, action, environment, gamma):\n",
        "    # Calculate the expected value of taking the action in the state under the optimal policy\n",
        "    expected_value = 0\n",
        "    for next_state, prob in environment.get_possible_next_states(state, action).items():\n",
        "        reward = environment.get_reward(state, action, next_state)\n",
        "        max_next_q = max(action_values[next_state].values())  # Find max Q-value under the optimal policy\n",
        "        expected_value += prob * (reward + gamma * max_next_q)\n",
        "\n",
        "    return expected_value  # This represents Q*(s, a) according to the Bellman Optimality Equation\n"
      ],
      "metadata": {
        "id": "xpPiqfRvDWUe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursive Relationship: The Bellman Optimality Equation establishes a recursive relationship between the optimal value of a state or state-action pair and the optimal values of its possible successor states or state-action pairs.\n",
        "\n",
        "Focus on Optimal Policy: It explicitly considers the optimal policy when evaluating expected values.\n",
        "\n",
        "Foundation for Algorithms: Dynamic programming methods like Value Iteration and Policy Iteration directly solve the Bellman Optimality Equation, while temporal difference learning algorithms like Q-learning indirectly converge to its solution."
      ],
      "metadata": {
        "id": "4q8xfZ5wDlAx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAb0oT9FQGDJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}